# -*- coding: utf-8 -*-
"""MnistData.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k4MPt3AcbvSYTNcJtISY38MojU-faeU_
"""

import tensorflow as tf
import matplotlib.pyplot as plt

(train_ds,train_label),(test_ds,test_label)=tf.keras.datasets.mnist.load_data()

plt.imshow(train_ds[1], cmap='gray')
plt.title(f"Label: {train_label[1]}")
plt.show()

train_ds = tf.data.Dataset.from_tensor_slices((train_ds, train_label)).batch(32)
test_ds = tf.data.Dataset.from_tensor_slices((test_ds, test_label)).batch(32)

IMG_SIZE = 28
BATCH_SIZE = 32
from tensorflow.keras import layers, models
resize_and_rescale = tf.keras.Sequential([
  layers.Resizing(IMG_SIZE, IMG_SIZE),
  layers.Rescaling(1./255)
])

data_augmentation = tf.keras.Sequential([
  layers.RandomFlip("horizontal_and_vertical"),
  layers.RandomRotation(0.2),
])

!pip install keras-tuner

from keras_tuner import HyperModel

class MyHyperModel(HyperModel):
    def build(self, hp):
        model = models.Sequential([
            layers.InputLayer(input_shape=(IMG_SIZE, IMG_SIZE, 1)),
            resize_and_rescale,
            data_augmentation,
            layers.Conv2D(
                filters=hp.Int('conv1_filters', min_value=16, max_value=64, step=16),
                kernel_size=hp.Choice('conv1_kernel_size', [3, 5]),
                padding='same',
                activation='relu'
            ),
            layers.MaxPooling2D(),
            layers.Conv2D(
                filters=hp.Int('conv2_filters', min_value=64, max_value=128, step=32),
                kernel_size=hp.Choice('conv2_kernel_size', [3, 5]),
                padding='same',
                activation='relu'
            ),
            layers.MaxPooling2D(),
            layers.Flatten(),
            layers.Dense(
                units=hp.Int('dense_units', min_value=64, max_value=128, step=32),
                activation='relu'
            ),
            layers.Dense(10, activation='softmax')
        ])

        model.compile(
            optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        return model

from keras_tuner import RandomSearch

tuner = RandomSearch(
    MyHyperModel(),
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='my_dir',
    project_name='my_project'
)

def split_dataset(ds, val_size=0.2):
    ds_size = len(ds)
    val_size1 = int(ds_size * val_size)
    train_size1 = ds_size - val_size1

    # Shuffle the dataset before splitting
    ds = ds.shuffle(10000, seed=123)

    val_ds = ds.take(val_size1)
    train_ds = ds.skip(val_size1)

    return train_ds, val_ds

# Apply the split
train_ds, val_ds = split_dataset(train_ds)

tuner.search(
    train_ds,
    epochs=10,
    validation_data=val_ds
)

model = models.Sequential([
    layers.InputLayer(input_shape=(IMG_SIZE, IMG_SIZE, 1)),
    resize_and_rescale,
    data_augmentation,
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

epochs=8
history = model.fit(
  train_ds,
  epochs=epochs
)